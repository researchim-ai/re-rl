{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение Qwen модели на искусственных задачах с GRPO\n",
    "\n",
    "Сделано на основе ноутбука от [unsloth](https://unsloth.ai/blog/r1-reasoning)\n",
    "\n",
    "Этот notebook показывает как обучать LLM на сгенерированных задачах из re-rl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth vllm tensorboard trl\n",
    "!pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "\n",
    "# Генераторы задач из re-rl\n",
    "from re_rl.tasks.generators import (\n",
    "    generate_random_linear_task,\n",
    "    generate_random_quadratic_task,\n",
    "    generate_random_futoshiki_task,\n",
    "    generate_random_knights_knaves_task,\n",
    "    generate_random_contradiction_task,\n",
    "    generate_random_urn_probability_task,\n",
    "    generate_random_arithmetic_task,\n",
    "    ALL_TASK_GENERATORS,\n",
    ")\n",
    "\n",
    "# Физические задачи\n",
    "from re_rl.tasks.physics.generators import (\n",
    "    generate_random_kinematics_task,\n",
    "    generate_random_quantum_task,\n",
    "    generate_random_physics_task,\n",
    "    ALL_PHYSICS_TASK_GENERATORS,\n",
    ")\n",
    "\n",
    "# Система наград\n",
    "from re_rl.rewards import (\n",
    "    reward_format_check,\n",
    "    reward_cot_quality,\n",
    "    reward_correctness,\n",
    ")\n",
    "\n",
    "# Промпты\n",
    "from re_rl.tasks.prompts import PROMPT_TEMPLATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация датасета для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re_rl.dataset_generator import DatasetGenerator\n",
    "\n",
    "generator = DatasetGenerator()\n",
    "\n",
    "# Генерация SFT датасета\n",
    "train_data = generator.generate_sft_dataset(\n",
    "    task_types=[\"quadratic\", \"linear\", \"kinematics\", \"quantum\", \"arithmetic\"],\n",
    "    num_samples=5000,\n",
    "    language=\"ru\",\n",
    "    difficulties=[3, 5, 7],\n",
    ")\n",
    "\n",
    "print(f\"Сгенерировано {len(train_data)} примеров для обучения\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка модели\n",
    "\n",
    "Используем LoRA для эффективного обучения на GPU с 24ГБ памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "max_seq_length = 2048\n",
    "lora_rank = 64\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Форматирование в chat формат\n",
    "def format_for_training(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{example['instruction']}\\n\\n{example['input']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": example['output']}\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Создаём датасет\n",
    "dataset = Dataset.from_list([format_for_training(ex) for ex in train_data])\n",
    "print(f\"Датасет готов: {len(dataset)} примеров\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции наград для GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"Извлекает ответ из текста.\"\"\"\n",
    "    patterns = [\n",
    "        r\"Ответ:\\s*(.+?)(?:\\n|$)\",\n",
    "        r\"Answer:\\s*(.+?)(?:\\n|$)\",\n",
    "        r\"=\\s*([\\d\\.\\-]+)\\s*$\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def reward_has_steps(text):\n",
    "    \"\"\"Награда за наличие шагов решения.\"\"\"\n",
    "    step_patterns = [\"Шаг\", \"Step\", \"1.\", \"2.\", \"3.\"]\n",
    "    has_steps = any(p in text for p in step_patterns)\n",
    "    return 0.5 if has_steps else 0.0\n",
    "\n",
    "def reward_has_answer(text):\n",
    "    \"\"\"Награда за наличие ответа.\"\"\"\n",
    "    answer = extract_answer(text)\n",
    "    return 0.5 if answer else 0.0\n",
    "\n",
    "def compute_total_reward(generated_text, expected_answer=None):\n",
    "    \"\"\"Вычисляет общую награду.\"\"\"\n",
    "    reward = 0.0\n",
    "    reward += reward_has_steps(generated_text)\n",
    "    reward += reward_has_answer(generated_text)\n",
    "    \n",
    "    # Проверка правильности если есть эталонный ответ\n",
    "    if expected_answer:\n",
    "        extracted = extract_answer(generated_text)\n",
    "        if extracted and str(expected_answer) in extracted:\n",
    "            reward += 1.0\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"./grpo_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    max_new_tokens=512,\n",
    "    num_generations=4,\n",
    ")\n",
    "\n",
    "# Примечание: полная настройка GRPO требует дополнительной конфигурации\n",
    "# функции reward_fn в соответствии с документацией trl\n",
    "print(\"Конфигурация GRPO готова\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Альтернатива: SFT обучение\n",
    "\n",
    "Если GRPO слишком сложен, можно начать с простого SFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=dataset,\n",
    "#     args=sft_config,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# trainer.train()\n",
    "\n",
    "print(\"SFT конфигурация готова\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_tasks(model, tokenizer, num_tasks=10):\n",
    "    \"\"\"Оценивает модель на случайных задачах.\"\"\"\n",
    "    correct = 0\n",
    "    \n",
    "    for _ in range(num_tasks):\n",
    "        # Генерируем случайную задачу\n",
    "        task = generate_random_quadratic_task(language=\"ru\")\n",
    "        result = task.get_result()\n",
    "        \n",
    "        prompt = f\"Решите задачу пошагово.\\n\\n{result['problem']}\"\n",
    "        \n",
    "        # Генерируем ответ модели\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Проверяем\n",
    "        expected = str(result['final_answer'])\n",
    "        if expected in generated:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / num_tasks\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{num_tasks})\")\n",
    "    return accuracy\n",
    "\n",
    "# evaluate_on_tasks(model, tokenizer, num_tasks=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./trained_model\")\n",
    "# tokenizer.save_pretrained(\"./trained_model\")\n",
    "# print(\"Модель сохранена!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
